Components
==========


Core Transformations
--------------------

When imported, ``blueetl`` automatically registers itself as a Pandas accessor, providing some custom methods to every Pandas DataFrame, Series, or Index.
These helper functions can be accessed using the ``etl`` namespace.

For example, if ``df`` is the variable containing a DataFrame, you can use ``df.etl.q()`` to call the ``q`` method, that provides an alternative way to query and filter the DataFrame.

The list of available methods can be found in the API documentation for ``blueetl.core.etl``.

For more information about Pandas custom accessors, see the `official documentation <https://pandas.pydata.org/pandas-docs/stable/development/extending.html#registering-custom-accessors>`__.


Simulation Campaign Configuration
---------------------------------

The class ``blueetl.config.simulations.SimulationsConfig`` is used to load a Simulation Campaign from a configuration file generated by ``bbp-workflow``.

Internally, it stores the list of simulations and their attributes as a Pandas DataFrame.


Spike Analysis
--------------

The class ``blueetl.analysis.Analyzer`` can be initialized with a specific configuration file, that should provide:

- the path to the simulation campaign configuration
- the parameters needed to extract the spikes from the simulations
- the parameters needed to calculate the features from the extracted spikes


Repository (Extraction)
-----------------------

The class ``blueetl.repository.Repository`` is responsible for the extraction of the spikes from the simulations.

It exposes the extracted data as objects wrapping the following Pandas DataFrames, that can be accessed directly if needed:

- simulations
- neurons
- neuron_classes
- trial_steps
- windows
- spikes


FeaturesCollection
------------------

The class ``blueetl.features.FesaturesCollection`` is responsible for the calculation of the features, using to the given configuration.

The configuration should specify how the spikes should be grouped and the name of a user defined function that's called for each group of spikes.

One or more dataframes of features can be produced, and they will be exposed by wrapper objects similar to the ones used for the repository.
